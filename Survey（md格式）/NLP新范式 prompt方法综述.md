学长您好，鉴于个人能力与时间原因，无法整理完survey并做出PPT，所有论文笔记如下。

# 大致目录

Part1 ： Prompt个人总结

Part2： 论文笔记



# Part1 

### survey：NLP新范式 prompt

与使用具体的分类器的传统Fine-tuning不同，基于prompt的fine-tune直接使用预训练的模型来执行分类或回归的预训练任务。

从 BERT (Devlin et al., 2019) 开始，在下游应用程序中使用特定任务的上游微调预训练语言模型 (LM) 已成为 NLP 的标准做法。 然而，具有 175B 参数的 GPT-3 模型带来了一种将 LM 用于下游任务的新方法：GPT-3 可以很好地处理少标注任务。 通过利用自然语言提示和任务演示作为上下文，GPT-3可以处理很大范围的任务，同时也不更新底层模型中的参数。 GPT-3 庞大的模型规模是其成功的重要因素，而提示和演示的概念也让我们对如何更好地使用语言模型有了新的认识。

## 1. prompt（提示）是什么？

​	prompt是插入到输入示例的一段文本。举个例子：“Obama is a  ____ (职业)”  ，这里的"____"就是prompt。

GPT-3 发布后，出现了很多与prompt相关的论文，其中很多都讨论了 BERT（BERT-base 有 110M 个参数，比最大的 GPT 小 1000 倍）等中等规模的预训练模型的基于prompt的学习）。



## 2. 为什么用prompt？

![image-20210926204356334](https://gitee.com/pinboy/typora-image/raw/master/img/202109262043408.png)

在标准的“pretrain+fun-tune”范式中，预训练阶段和下游任务之间的差距可能很大、

对于下游任务，我们通常需要引入新的参数。例如，对于 BERT 大型模型和二元分类任务，它需要一组额外的 1,024 x 2 参数。但是prompt使得下游任务可以采用与预训练目标相同的格式（如上图所示）并且不需要新的参数！

对于分类任务，我们只需要设计一个模板（“It was”）和预期的文本响应（我们称这些标签词，例如图中正标签为“great”，负标签为“terrible” ）。

通过缩小这上下游两个阶段之间的差距，在特定任务上部署预训练模型变得更加容易，特别是对于少样本的情况

当你只有十几个训练样本来完成一项新任务时，很难精确而有效地调整预训练模型和新的特定于任务的参数，但如果换成prompt方法则要顺畅得多，因为你压根就不需要引入新的参数。 研究表明一个prompt可能价值 100 个常规数据点，这说明prompt可以样本效率产生巨大提升。



## 3. 怎么用prompt？

prompt的研究有两种不同的范式：

1. 基于prompt的微调（关键点是进一步优化参数 ) 被认为是通往更好的小语言模型的小样本学习者的途径

   这种方法受 PET 论文的启发 ([Schick and Schütze, 2021a](https://arxiv.org/abs/2001.07676),[ b](https://arxiv.org/abs/2009.07118)),（小的，我的意思是数百万而不是数十亿的参数，如 BERT 或 RoBERT； ）

2. 直接使用prompt来调整参数并用于不同的下游任务中

   因为对于像 175B GPT-3 和 11B T5  这样的超大型模型，微调它们是很困难的而且成本很高， 所以我们希望通过不同的prompt（离散的或连续的，我将在后面讨论）来调整它们的参数并应用它们到不同的任务。

   

两种不同形式的prompt

1.  离散的提示（**Discrete prompts**）

   离散的，顾名思义就是自己设计一个prompt或者生成一个prompt，一个一个地应用到训练中。

   GPT-1/2 适当设计prompt，LM在情感分类和阅读理解任务上有不错的性能

   在LM利用prompt可以挖掘事实和常识，后来prompt方法被引入了小型LM，与大型LM如GPT-3不同，小LM对完整模型进行了微调，并且用的是双向掩码。合适地在prompt基础上进行微调可以让模型性能进一步优化。

   大部分的工作都是基于人工设计prompt，但这样其实费时费力又无法达到最优。也有做了自动搜索prompt的工作，比如 [Guo et al., 2021](https://arxiv.org/abs/2106.07704) 表明 soft Q-learning方法对prompt迭代有很好的效果。AutoPrompt ([Shin et al., 2020](https://arxiv.org/abs/2010.15980)) 可以基于梯度来进行搜索。

   AutoPrompt就像做了一个整合，里面有很多的Template，基于输入和Trigger Tokens，可以输出一个推荐的prompt。

   ![image-20210926211645256](https://gitee.com/pinboy/typora-image/raw/master/img/202109262116324.png)

   

2. soft prompt（不知道怎么翻译了）

   因为 AutoPrompt 已经对提示进行了基于梯度的搜索，所以我们可以从离散标记转向连续的“soft prompt”了。

   

## 4. 上下文学习：一种新的元学习形式

​	GPT-3的成功可以归结于两点：Prompts和上下文学习。

GPT-3其实没有为下游任务微调参数，但是它可以通过上下文来学习下游任务的知识。

![image-20210926212305680](https://gitee.com/pinboy/typora-image/raw/master/img/202109262123754.png)



如图，GPT3从训练集中，将随机实例(sea otter=> loutre de mer) 和 实际查询（比如这里的cheese=>) 连接起来。

由于预训练模型已经学会从上下文中捕获模式，并且 Transformers 的自注意力允许跨过这些实例逐个进行比较，因此上下文学习的效果出奇地好。

GPT-3就称之为“元学习”，即“阅读”了大量的无监督文本数据后，模型初步具有广泛的模式识别能力。GPT-3作者也说，预训练期间也会在单个序列中嵌入重复的子任务，很类似与上下文学习的范式。

 [Gao et al., 2021](https://arxiv.org/abs/2012.15723); [Liu et al. (2021)](https://arxiv.org/abs/2101.06804) 也表明，按照上下文给模型展示相关查询信息，会比随机展示查询信息效果更好[Lu et al. (2021)](https://arxiv.org/abs/2104.08786) 表明演示的顺序很重要，并提出了一种新的定义最优展示顺序。

虽然in-context learning只有在不能调优模型时才“需要”，当训练样例数量增加时很难泛化（因为模型的输入长度有限），研究如何更好地使用演示（即 ，如何进一步压缩 LM 学到的“元知识”）以及哪些预训练目标和数据可以提高上下文能力，同时这样可能会进一步帮助我们了解预训练 LM 的内部工作原理。





# Part2

----

### 论文

主要阅读了https://github.com/thunlp/PromptPapers中的文章，了解了prompt的基本思想与应用，部分论文摘抄笔记如下。

### 论文分类：

Prompt综述

Prompt的设计

Prompt的评估

Prompt的应用



# [【0】Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference](https://aclanthology.org/2021.eacl-main.20/)

> 提出了Pet :  小模型半监督，匹配下游任务。
>
> 其中总结了若干prompt的pattern

## 综述

这篇论文提出cloze-style prompt-based fine-tuning方法Pet（Pattern Exploiting Training）

Pet是一种半监督的训练程序，将输入示例重新定义为cloze-风格的短语，以帮助语言模型理解给定的任务

这些短语被用来为大量未标记的示例分配soft标签。最后，对所得到的训练集进行标准的监督训练。

对于几种任务和语言，PET在低资源设置下的性能大大优于监督训练和强半监督方法



## 简介

**<u>Learn from examples</u>**：一个模型在一组有标记的例子上进行训练，然后从中将其推广到看不见的数据。

考虑到语言、领域，任务的不同，以及注释数据的成本，在NLP中少标签训练语料是常见的。这让基于少量标签训练成为一个非常重要的研究领域。

**<u>下游任务描述</u>**：

但是，将标准的监督学习应用于小的训练集通常表现不佳。（训练样本太少）

但是，如果我们清楚了下游任务描述后，对于小样本进行训练变得简单得多。

随着GPT(Radfordetal.)、BERT(Devlin等人，2018)、Devlinetal.，2019)和RoBERTa(Liuetal.，2019)的兴起，为神经结构提供任务描述的想法已经变得可行。

在本文中，作者成功地将任务描述和小样本标准监督学习结合在一起

## pet的工作原理

PET分三个步骤工作：

首先，对于每个模式，在一个小的训练集T上完成一个单独的PLM。

然后，所有模型的集成被用于用软标签标注一个大的未标记数据集D。

最后，在软标记数据集上训练一个标准的分类器。作者还设计了iPET，这是一种PET的迭代变体，其中这个过程随着训练集大小的增加而重复。

![image-20210926111759459](https://gitee.com/pinboy/typora-image/raw/master/img/202109261117508.png)



## pattern

- Yep

  ![image-20210926203123890](https://gitee.com/pinboy/typora-image/raw/master/img/202109262031944.png)

- **AG’s News**

  ​	                                     ![image-20210926203151521](https://gitee.com/pinboy/typora-image/raw/master/img/202109262031572.png)

- **Yahoo** 

- **MNLI** 

- **X-Stance**

## 结论

- 提供任务描述的预训练模型可以与标准的监督训练相结合。
- 当初始训练数据量有限时，PET比标准监督训练和强半监督应用有很大的改进



# [【1】It’s Not Just Size That Matters:Small Language Models Are Also Few-Shot Learners](https://aclanthology.org/2021.naacl-main.185.pdf)

> 使用了pet的语言模型：小模型少参数抗衡GPT-3



最近的预训练模型参数已经扩展到千亿个级别了，如GPT-3 。 很多时候训练这样一个巨大的模型带来的资源消耗也是不可低估的，这就给研究者的使用带来困难。

但是不是参数越多，模型越大性能就越好。改善后的语言模型仍然可以有类似的性能，但是参数数量却小上很多。

这篇论文将文本输入转换为包含任务描述的封闭问题，并结合基于梯度的优化来实现的；利用未标记的数据可以提供进一步的改进。通过小语言模型确定了成功理解自然语言所需的关键因素。

![image-20210926110925838](https://gitee.com/pinboy/typora-image/raw/master/img/202109261109902.png)

​																							参数更少，性能更优

主要使用的方法就是上面一篇论文提出的Pet方法，并完成实验验证：在文本分类任务上，更小参数数量级的pet可以比GPT有更优良的性能

![image-20210926105528738](https://gitee.com/pinboy/typora-image/raw/master/img/202109261055783.png)



# 【2】[Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing](https://github.com/thunlp/PromptPapers)	1

> 一篇很好的关于prompt的综述

![image-20210926101339250](https://gitee.com/pinboy/typora-image/raw/master/img/202109261013297.png)



## NLP的四种范式

1. Fully Supervised Learning (Non-Neural Network)
2. Fully Supervised Learning (Neural Network)
3. Pre-train, Fine-tune
4. Pre-train, Prompt, Predict

>  全监督学习，即仅在目标任务的输入输出样本数据集上训练特定任务模型，长期以来在许多机器学习任务中发挥着核心作用，同样的，全监督学习在 NLP 领域也非常重要。但是全监督的数据集对于学习高质量的模型来说是不充足的，早期的 NLP 模型严重依赖特征工程。
>
> 随着用于 NLP 任务的神经网络出现，使得特征学习与模型训练相结合，研究者将研究重点转向了架构工程，即通过设计一个网络架构能够学习数据特征。
>
> 然而，从 2017-2019 年开始，NLP 模型发生了翻天覆地的变化，这种全监督范式发挥的作用越来越小。
>
> 具体而言，研究重点开始转向预训练、Fine-tuning范式。在这一范式下，一个具有固定架构的模型通过预训练作为语言模型（LM），用来预测观测到的文本数据的概率。由于训练 LM 所需的原始文本数据需要足够丰富，因此，这些 LM 都是在比较大的数据集上训练完成。之后，通过引入额外的参数，并使用特定任务的目标函数对模型进行Fine-tuning，将预训练 LM 适应于不同的下游任务。在这种范式下，研究重点转向了目标工程，设计在预训练和Fine-tuning阶段使用的训练目标（损失函数）。
>
> 当前我们正处于第二次巨变中，「预训练、Fine-tuning」过程被称为「预训练、prompt 和预测」的过程所取代。在这种范式中，不是通过目标工程使预训练的语言模型（LM）适应下游任务，而是重新形式化（Reformulate）下游任务，使其看起来更像是在文本 prompt 的帮助下在原始 LM 训练期间解决的任务。通过这种方式，选择适当的 prompt，该方法可以操纵模型的行为，以便预训练的 LM 本身可以用于预测所需的输出，有时甚至无需任何额外的特定任务训练。这种方法的优点是给定一组合适的 prompt，以完全无监督的方式训练的单个 LM 就能够用于解决大量任务。然而该方法也存在一个问题——这种方法引入了 prompt 挖掘工程的必要性，即需要找出最合适的 prompt 来让 LM 解决面临的任务。



## prompt 是什么？ 

prompt释义： 提示，提词器 。   简单的理解，就是遮住一些词汇或句子，作为训练资料喂给机器。

Prompt的形式：

1. cloze prompt（填充空白字符串）
2. Prefix prompt （前缀）

选择哪一个取决于任务和用于解决任务的模型。一般来说，对于有关生成的任务或使用标准自回归 LM 解决的任务，前缀 prompt 往往更有帮助，因为它们与模型从左到右的性质刚好吻合。对于使用掩码 (Mask) LM 解决的任务（比如，BERT），完形填空 prompt 则非常合适，因为它们与预训练任务的形式非常匹配。全文本重建模型则可以与完形填空 prompt 或前缀 prompt 一起使用。最后，对于一些涉及多个输入的任务，例如文本对分类，prompt 模板必须包含至少两个输入的空间。

## prompt的模板如何定义？

1. 手动定义

   直观，但是需要大量经验和时间，而且也无法最优化

2. 自动生成

## **multi-prompt 方法**

复合 prompt 可以进一步提升 prompting 方法的效果

下面试常见的方法：

1. prompt ensembling

   把多个prompt通过某种加权方法组合到一起

2. prompt augmentation

   启发式学习，如图

3. prompt composition

   将复合的prompt句子，拆解成多个小段prompt，最后再组合在一起训练

4. prompt decomposition

   由于一些任务的mask工作使用句子数量有限（比如词性标注任务），于是就只能通过decomposition将一个句子拆分成多个部分后，再对每个部分做prompt单独训练

![image-20210926102917042](https://gitee.com/pinboy/typora-image/raw/master/img/202109261029111.png)



## answer空间的设计

如果说prompt是将语料空间A映射到一个隐藏空间B，那么answer就是将隐藏空间B映射到结果空间C，answer可以理解为任务对话系统里面的回答库，就是机器该回答的内容所构成的集合。

### answer集合如何定义？

#### 			定义answer的形式

​				answer 的形式决定了它的粒度，一些常见的选择包括

- Token：预训练 LM 词汇表中的一个 token，或者词汇子集；
- Span：短的 multi-token span，这些通常与 cloze prompt 一起使用；
- 句子或文档：这些通常与前缀 prompt 一起使用

  #### 搜索answer的方式

  ​	和prompt一样，需要一个模板，模板的定义也分为人工和自动生成。

## **prompting 方法的训练策略**

- Promptless Fine-tuning

- Tuning-free Prompting

- Fixed-LM Prompt Tuning

- Fixed-prompt LM Tuning

- Prompt+LM Fine-tuning

![image-20210926104304638](https://gitee.com/pinboy/typora-image/raw/master/img/202109261043691.png)

## prompt的应用

- 知识探索（事实探索和语言学探索）
- 分类任务（文本分类和自然语言推理）
- 信息提取（关系提取、语义分析和命名实体识别）
- NLP 中的推理（常识推理和数学推理）
- 问答
- 文本生成
- 文本生成的自动评估
- 多模态学习
- 元应用（域自适应、除偏和数据集创建）







# [【3】Parameter-Efficient Transfer Learning for NLP](http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf)



> 提出一个适配器来代替对不同下游任务的Fine-tune工作，参数精简而有效

这篇论文主要是提出了一个适配器来配合迁移学习，而且通过实验验证了这个适配器是非常高效的。



Fine-tuning+大规模预训练模型是NLP中有用的范式。

然而，在存在许多下游任务的情况下，Fine-tuning是效率低下的：每个任务都需要一个全新的模型。

作为一种替代方案，我们建议使用适配器模块进行迁移。适配器模块产生一个紧凑和可扩展的模型；它们每个任务只添加少数可训练参数，可以添加新任务而无需重新访问以前的任务。原始网络的参数保持不变，产生高度的参数共享。

为了证明适配器的有效性，我们将最近提出的BERT transformer模型迁移到26个不同的文本分类任务中，包括GLUE基准测试。适配器获得了接近最先进的性能，而每个任务只添加几个参数。在GLUE上，我们获得了完全Fine-tuning性能的0.4%范围内，每个任务只添加3.6%的参数。相比之下，Fine-tuning训练每个任务100%的参数。



![fun](https://gitee.com/pinboy/typora-image/raw/master/img/202109261609120.png)

​																	适配器和fun-tune的效果对比











# [【4】How Can We Know What Language Models Know?](https://arxiv.org/pdf/1911.12543.pdf)

>  自动Prompt选取

## 综述

一般的prompt是人工创建的，比如 “Obama is a  ____ (职业)” ，这个坑是人工挖出来的，如此训练，这个模型就能够很好地去预测某人的职业。然而另一个方面的问题是，如果prompt没有设定好，很可能就无法准确地预测LM所包含的知识。

本文提出了一个方法，自动优化prompt的选取，从而更加准确地获取到LM中学习到的知识。具体来说，作者提出了基于挖掘和基于解释的方法来自动生成高质量和多样化的prompt，以及集成方法来组合来自不同提示的答案。

## LM的发展

近年来，语言模型(LM)的主要作用已经从生成或评估自然文本的流畅性转变(Mikolov和茨威格，2012；Merity等人，2018；Melis等人，2018；Gamon等人，2005)到成为文本理解的强大工具。这种理解主要是通过使用语言建模作为特征提取器的训练前任务来实现的，通过语言建模目标学习的隐藏向量随后被用于下游语言理解系统。

现在LM本身已经可以作为文本理解的工具，可以用于

- 自然语言构建查询，
- 直接生成文本答案
- 评估多种选择和选择最优选择
- 回答常识问题
- 提取实体之间关系

无论最终任务是什么，LM中包含的知识都可以通过提供prompt来探索答案，prompt可以是前缀形式，也可以是cloze形式

虽然现在prompt范式已经应用于LM并取得了一些效果，但是手动创建prompt并不是最理想的，因为LM学习到的目标知识可能来自于不同的环境，而且LM的知识很可能由于设计者无法考虑到有效的信息而无法被检索到。

因此，现有的结果只是LM中所包含的知识程度的一个下界，事实上，LM含有的知识程度可能比这些初始结果更高。

所以问题是，我们怎么去收紧这个知识程度的下界，并获得对LM所包含知识得一个更好的估计方法。

![image-20210926145341333](https://gitee.com/pinboy/typora-image/raw/master/img/202109261533956.png)



如何自动提取实体之间的关系呢？下面是两种自动的方法来系统地提高用于查询一个关系的存在性的提示的广度和质量

## 基于挖掘的prompt生成方法

- **Middle-word Prompts** 

在（主语）和（宾语）之间的词往往表明了这种关系之后，我们直接使用这些词（主宾）作为prompt。

例如，“**奥巴马** 出生在**夏威夷**”通过用占位符替换主语宾语，被转换为提示的“x出生在y中”。

- **Dependency-based Prompts**

可能关系出现的地方，不是在主宾中间。（eg：*The capital* *of France is <u>Paris</u>*” 中的Paris）

那怎么找到<u>Paris</u>作为prompt呢？

本文使用依赖解析器解析句子以确定主语和宾语之间的最短依赖路径，然后使用从依赖路径中最左的单词到最右的单词的短语作为prompt。

![image-20210926153809767](https://gitee.com/pinboy/typora-image/raw/master/img/202109261538972.png)





## 基于解释的prompt生成方法

​	这种方法更加有针对性-它的目的是提高词汇多样性，同时保持相对忠实于原始的提示。

通过将原始提示符转译为其他语义上相似或相同的表达式来实现这一点。

例如，如果我们的原始提示符是“X share a border with Y”，它可以解释（翻译）为“x与y有一个共同的边界”或者说“x邻接y”。这在概念上类似于信息检索中使用的查询扩展技术，该技术可以重新制定给定的查询以提高检索性能。





























# [【5】Language Models as Knowledge Bases?](https://arxiv.org/pdf/1909.01066.pdf)

> 这是Facebook AI research的一篇论文，将LM作为“知识”的基石，基于Bert提出了在公开的预训练模型下的 事实与常识知识系统Bert-large，并鼓励大家使用bert-large作为研究的起点。

## 综述

在大型文本语料库上的预训练语言模型的最新进展导致了对下游NLP任务的大量改进。在学习语言知识的同时，这些模型也可能存储训练数据中存在的关系知识，并可能能够回答结构为“  句子+ <u>Block</u>”类的查询（prompt）。

与结构化知识库相比，语言模型有许多优点：

- 它们不需要模式工程
- 允许从业者查询开放关系类
- 容易扩展到更多的数据
- 并且不需要人工监督进行训练

对各种最先进的预训练语言模型中已经存在（没有微调）的关系知识进行了深入的分析。发现

(i)在没有微调的情况下，BERT包含的关系知识包含了一些知识，

(ii)BERT在针对监督基线的开放领域问题回答上也做得非常好

(iii)学习某些类型的事实知识，相比于标准的预训练模型流程，会更容易学习。

而且这些模型在不进行任何微调的情况下回忆事实知识的能力非常强。这表明了它们很可能应用于无监督开放域QA系统。



图：Bert-large 的生成任务 例子

Bert-large 随机使用不同的语言模板，Bert-large仍然能够预测出正确的类型，即使模板本身并不适合这个句子。

![image-20210926184243086](https://gitee.com/pinboy/typora-image/raw/master/img/202109261842167.png)





## 背景：

ELMo和Bert等大规模预训练模型的出现，对下游的任务模式产生了极大的影响。可以通过调节上游模型的潜在上下文表示，或使用上游的模型的权值，来初始化针对特定下游任务的模型，然后进一步微调。这样的迁移学习的模式对如今的下游任务处理很重要。

相比之下，知识库能够很好地解决直接从定义好规则的数据库中去查询数据的问题，但是一般都是从text或者是其他文件读取信息移植到知识库中，这种方式就需要各种复杂的管道和解码方式，错误很容易累加。

所以我们现在为什么不直接从模型本身去抽取知识呢？就直接问LM：“Daniel is born in __ _？” 叫LM来填一个空，就能够获取到我们的知识了。再举个例子，对于情感分析任务，可以问LM：“today i have a cake，it is __ _” ，就能够获得我们所需要的情感词汇了。

但是对于ELMo和Bert这样的大规模模型，我们需要问：它们存储了多少知识？不同的知识之间存储的方式有什么不同？如果没有进行微调，它们与传统的直接从text中读取知识所获得的结果有什么差异？

为了解决上面的问题，Facebook提出了*LAMA* (LAnguage Model Analysis) probe,  来检测模型。得出的结论是：

1. Bert-large提取知识，基本结构类似与 知识库的关系提取器+语料库的实体链接器。
2. 事实类的关系（有唯一答案的，相当于N-1映射）可以被很好地提取出来，但是N-M映射类的知识，却无法被很好地表达。
3. Bert-large在查询事实与常识知识方面的性能优于其他模型，同时查询的方式更加健壮。
4. 在开放域对话系统中Bert-large精确度再创新高

## 结论：

Facebook提出了一个基于公开的预训练语言模型的 事实和常识知识系统bert-large。bert-large有更好的知识捕捉能力，并且有能力代替非神经和监督学习。这篇文章里，并没有比较bert-large捕获知识的能力，而是关注与模型内部存在知识本身，Bert-large可以成为研究人员工作的起点。

从标准的文本训练模式转变到使用预训练好的Bert-large模式是很简单的。为了更好地训练bert使其有优势，bert-large训练集增加了 Wikitext-103 （可能是一个很大的数据集）。这样其实再用更多的数据可能也不再会对关系解释的性能有较大的提升，但是基于逐渐增长的大规模语料库的预训练模型，在未来很可能成为知识关系抽取的基石。

除了使用LAMA prope来测试未来的预训练语言模型外，我们未来还会量化不同自然语言模板之间在知识提取性能上的差异。此外，评估复合token的答案仍然是一个挑战。







# [【6】What Makes Good In-Context Examples for GPT-3?](https://arxiv.org/pdf/2101.06804.pdf)

> 提出了 KATE：一种非参数检索方法，仅根据与测试语义的相似度来检索上下文。
>
> GPT-3的能力依赖于上下文例子选取，基于prompt与KATE更好地选取上下文示例，效果优于随机抽样基线。



## 综述

GPT-3由于其在广泛的NLP任务中的出色表现，特别是其少标注和多情境学习能力，引起了大量关注，但我们发现GPT-3的实验结果在很大程度上依赖于上下文例子的选择。

作者研究了一种更有效的策略来选择上下文示例（相对于随机抽样），以更好地利用GPT-3的少样本学习能力。

用检索模块可以增强大规模神经网络，受到其启发，我们基于检索语义上与测试样本相似度来选取上下文例子，以此表示对应prompt。直观地说，使用这种策略选择的上下文示例可以获得更多的信息输入，来扩展GPT-3的知识。

我们在几个自然语言理解和生成基准上评估了所提出的方法，其中基于检索的提示选择方法始终优于随机基线。

## 结论

提出了 KATE：一种非参数检索方法，仅根据与测试语义的相似度来检索上下文。

用这种方法能够提高GPT-3的性能，显著优于随机抽样基线。此外，在任务相关的数据集上对词嵌入进行微调可以进一步提升性能。同时进行模型简化测试，来探究KATE方法的影响。



















# -----Application

# [【7】Thinking Aloud: Dynamic Context Generation Improves Zero-Shot Reasoning Performance of GPT-2](https://arxiv.org/pdf/2103.13033.pdf)

>  一篇prompt的具体应用文章: 应用prompt于GPT-2训练，效果良好

## think aloud ：

不是大声思考的意思，而是“自言自语”，在这里指的是基于元认知，类似人类用推理思维，解决一些困难的问题的技术。

## 综述：

think-aloud 在这里指的是基于元认知，类似人类用推理思维，解决一些困难的问题的技术。基于think-aloud，可以提高预训练神经网络模型的推理能力，即通过由语言模型本身动态生成的问题阐述来扩展任务的上下文。动态生成问题的细化，显著提高了GPT-2在演绎推理和自然语言推理任务中无样本学习的能力当使用启发式训练方法来预测答案的时候，模型也会生成一些额外的上下文。利用好这些额外的上下文，有助于训练的效果提升。此外，细化的有效性可以用细化在语义上与相应问题一致的程度来解释。



## Introduction

Transformer，Bert，Bart，GPT-3，这些技术不仅正在重塑NLP领域，而且可能对我们在学术界(特别是人文和社会科学领域的阅读、研究和写作方式产生深远的影响。

想象一下，当一个孩子遇到一个不会的问题时，但是必须给出答案之前，她可能会沉默地思考或自言自语。实际上，研究表明，自言自语的经验可以提高儿童解决问题的能力和阅读理解能力。这样的过程对大人同理，对机器也许也是一样。

下面就将“自言自语” 这个词汇，换成“dynamic problem elaboration  /动态问题解释”，指的是解决问题的关键文本信息。



下面是一个例子：

（像是从一段文本里面做推理，然后根据文本逻辑去分析，Marian is **，（ * *为预测内容））

中间的prompt内容，就是逻辑推理的关键之处，也是模型需要训练的地方。当prompt的地方训练好以后，模型的逻辑推理能力和语义理解能力自然就会提升了。

![image-20210926120448412](https://gitee.com/pinboy/typora-image/raw/master/img/202109261204472.png)



## **Dynamic Templating and Context Retrieval**

动态模板是指用于构建给定数据的自然语言模板的自动和特定大小例的优化查询。具体来说，Jiang等人。[2020]探索了改进人工生成提示的三种策略：从数据库中挖掘有效提示（例如，维基百科）、转译提示（例如，通过双向翻译、来回转换），以及汇集多个提示。这些策略都被证明可以显著改善QA任务的预测



## 如何生成问题的elaboration 呢？

（这里的elaboration 指的应该就是思考问题，解决问题的关键逻辑部分）

![image-20210926122035384](https://gitee.com/pinboy/typora-image/raw/master/img/202109261220443.png)

1. *Free elaboration*

首先教给机器相关的规则和词汇意思，接下来在给出的文本中 ，提出问题而不给出回答，让机器来自动给出回答。

>  例子：: "Here is what we know: |---被prompt掉的内容---|Does this mean that Loretta is not hungry, is hungry, or is not full? Explain!"

同样，还有下面这几种生成方法，不再赘述。

2.  fewshot PC
3. structured （piecemeal）
4. fewshot IC
5. fewshot PCIC
6. recursive（piecemeal）



## 效果和问题

GPT-2基于一个简单的启发式模型，利用规则推导问题，并使用prompt方法，在某些问题上可以达到很高的准确率。

但是一旦造成了systematic bias或者*effective distraction*太高的话，效果就不是很好了。







# [【8】**GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation**](https://arxiv.org/pdf/2104.08826.pdf)

> 应用prompt在GPT3Mix上，首次提出使用基于prompt的方法从大规模语言模型中生成合成样本，以实现文本增强

GPT3这样的大规模语言模型在few-shot训练上表现良好，同时也能够通过不同的prompt方式来控制。最近研究显示基于prompt分类可以免去fun-tune的需要，但是这种方式缺乏训练集，而且灵活性不够。

本文提出了一种新的数据增强技术，利用大规模语言模型从真实样本的混合中生成真实的文本样本。同时，利用好预训练好的大规模LM预测得到中间结果soft-labels，可以有效地从大规模LM提取知识，并且还能得到 文本干扰。

我们在不同的分类任务上进行了数据增强实验，并表明我们的方法大大优于现有的文本增强方法，同时模型简化测试和定性分析的研究为我们提供了更好的参考。



## 背景：

已被被证明，大规模的语言模型（比如GPT-3），通过基于prompt的上下文学习，在无标注和少标注的学习任务上性能更优。

上下文学习利用一个prompt，它通常包括一个任务描述和几个例子，来解决隐藏的任务，而不需要高昂的微调代价。认识到情境学习和基于prompt的控制的潜在研究应用，NLP社区的一部分已经将重点转移到理解和设计优化基于prompt的方法的先进方法上。然而，这些在大规模语言模型上的推理方法存在一些缺点。

首先，上下文训练语料的数量被现有prompt模板的数量限制。其次，基于prompt的方法需要在昂贵的大规模LM进行线上学习，但是这样的问题是模型过大内存吃不消，而且无法扩展。最后，基于prompt的方法和如今大火的fun-tune方法不兼容，还需要大家的探索。



为了解决这个问题：本文使用了GPT-3作为语料生成器输出soft-label并注入训练集，接着用Bert训练![image-20210926170428969](https://gitee.com/pinboy/typora-image/raw/master/img/202109261704007.png)

所以怎么生成语料呢？

文中提出了GPT3Mix，这是一种利用GPT-3等大规模语言模型从混合真实数据中合成超现实文本的方法。

GPT3Mix使用了大规模语言模型预测得到的soft-label来抽取知识，从而实现数据增强和知识蒸馏。





## 知识蒸馏：

知识蒸馏,是一种直接生成一个子分类器的技术。形象一点说，就像一个老师教学生，一个大规模的模型可以作为“老师”分类器，来生成一些小规模的“学生”分类器。这种知识传授的过程，被称之为知识蒸馏。

在模型压缩的背景下，语言模型的知识蒸馏已经在文献中得到了很好的研究。针对预先训练好的语言模型，已经提出了各种蒸馏模型和蒸馏方法。通过利用大规模语言模型预测的soft-labels，本方法有助于将知识转移到下游分类器。

## 文本增强：

文本增强是指在不改变类标签的情况下对语言空间进行扩增的方法（在当前语言空间下生成更多的文本），以提高下游模型的鲁棒性和通用性。数据增强已经在NLP现场得到了广泛的研究。在目前的文献中，文本增强有两种方向：浅层增强和深度增强。浅层数据增强技术向语言空间（单词或短语）注入局部可信的小噪声，希望扰动在保持标签一致性的同时产生语言上可接受的样本。

另一类增强技术使用外部语言模型来提高全局的一致性和一致性。反向翻译方法利用翻译语言对中的语义一致性来生成新的释义。在最近的研究中，预先训练的语言模型，如BERT，或seq2seq-BART，被用于获得更多样化和语义正确的增强样本。例如，BART已被证明在为数据稀缺的标签填充文本样本方面是有效的。

另一方面，受针对视觉领域提出的混合技术的启发，也有人使用统计方法混合现有文本样本，生成真实的增强文本，即用模型预测注释未标记数据的行为，这种方法已被用于设置半监督学习。



## 大规模语言模型

Devlin等人使用Pre-trained transformer-based language models在NLP领域掀起了一阵热潮，创造了一个NLP的新范式。

随着最近大规模语言模型的发展，我们正在见证该范式的另一个转变，即基于prompt的NLP。这些大型语言模型本质上是少标注的大规模已训练好的训练网络，而且它们可以通过自然文本来控制。

本文的工作依赖于之前基于prompt的方法。据我们所知，这项工作是首次提出使用基于提示的方法从大规模语言模型中生成合成样本，以实现文本增强

## conclusion:

本文提出了一种新的文本增强技术GPT3Mix，它利用大规模语言模型及其通过prompt以控制生成内容的方向。

我们在各种分类任务上的广泛实验表明，增强样本可以提高预训练的Transformer模型的鲁棒性和分类性能，

实验表明，基于GPT-3的数据增强方法在未来可以取代 基于prompt的特定任务方法和直接fun-tune的方法。







