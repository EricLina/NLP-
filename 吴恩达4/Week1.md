## 1.1 训练 开发 测试集

- 机器学习中，数据集分为三类： 训练，验证，测试集

- 训练集与测试集分布不匹配： 确保训练与测试数据来自同一分布

## 1.2 偏差与方差



1.2.1 

偏差过大（数据欠拟合），正好合适，过拟合 的例子

![image-20210915163323638](https://gitee.com/pinboy/typora-image/raw/master/img/202109151633786.png)



1.2.2 

**偏差**

是指训练结果的误差大小

- 最优误差也被称为贝叶斯误差（就是效果最好的那个偏差）（与分析方法，样本质量有关）

**方差**

训练集的误差：α  ；  验证集的误差 β ；

方差variance就是描述 α与β之间的差别度



如果训练集合的误差很小，但是验证集合的误差比较大，那可能是过拟合了数据集（某种程度上没有充分利用交叉验证集的作用），此时二者偏差很大， **高方差**

训练集的误差很大，验证集的误差也很大（但是与训练集的偏差相差无几），则称之为**数据欠拟合**，但是由于偏差相差不大，所以方差不高。

训练集的误差很大，验证集的误差更大，此时是高偏差，高方差。

当然，理想情况就是两个误差都很小，此时是低方差，低偏差。

![image-20210915164057517](https://gitee.com/pinboy/typora-image/raw/master/img/202109151640582.png)

通过Train set error ：看出数据拟合情况

通过Dev set error：结合训练集误差，可以判断方差是否过高





总结：通过训练集合验证集的误差，来判断算法偏差或者方差是否过高



## 1.3 机器学习基础

1.3.1 

偏差很高：（无法拟合数据）

1. 选择一个新网络
2. 或者花费更多时间训练

检查方差，如果太大

1. 更多的数据
2. 正则化
3. 新的网络

总之要多尝试，知道得到一个低方差，低偏差的模型。

1.3.2

偏差和方差的权衡

只要持续训练更大的网络，更多的数据，合适的正则化，现在能够在同时减少偏差和方差而不会相互影响。



## 1.4正则化

解决高方差的一个主要办法是正则化，还有可以准备更多数据

$$J = -\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} \tag{1}$$ To: $$J_{regularized} = \small \underbrace{-\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} }_\text{cross-entropy cost} + \underbrace{\frac{1}{m} \frac{\lambda}{2} \sum\limits_l\sum\limits_k\sum\limits_j W_{k,j}^{[l]2} }_\text{L2 regularization cost} \tag{2}$$



![image-20210915190232167](https://gitee.com/pinboy/typora-image/raw/master/img/202109151902254.png)

![image-20210915170726289](https://gitee.com/pinboy/typora-image/raw/master/img/202109151707353.png)

能够减少拟合误差

- 关于，深度学习中L2范数为啥能减少过拟合，直观上来说，若过大，则权重矩阵W接近于0，把多个隐藏层单元权重设为0，几乎消除了这些隐藏单元的许多影响，这个被大大简化的网络会变成一个很小的网络，小到如同一个逻辑单元，但是深度却很大。直观上当很大时，W会接近于0，我们尝试消除或至少减少许多隐藏单元的影响，最终网络会变得更简单，这个网络越来越接近逻辑回归，我们直觉上认为大量隐藏但愿被完全消除了，其实并没有，该神经网络的所有隐藏单元依旧存在，但是他们的影响变小了，这样不容易发生过拟合。
- L2范数会使W的值不会过大，当W较小的时候，Z就可能比较小，这里就可以利用双曲正切函数的线性状态，当W变大时，z就可能变大，激活函数就开始变得非线性。可能这部分值就集中在激活函数比较线性的部分，非线性不那么明显，所以简化了模型。
- 因为正则化在之前的cost function上加了一个正则项，使得当λ足够大时W矩阵足够小，这样 <img src="https://gitee.com/pinboy/typora-image/raw/master/img/202109151709145.png" alt="image-20210915170935115" style="zoom: 50%;" /> 就足够小，当Z足够小时，激活函数g(Z)（比如tanh函数)只用了其线性部分，接近于线性函数的激活函数的组合就会接近于线性函数，模型简单，就不会过拟合。总会存在一个适合的λ使得模型拟合函数刚刚好。



## 1.5 DropOut正则化

![image-20210915172755600](https://gitee.com/pinboy/typora-image/raw/master/img/202109151727653.png)

遍历网络的每一层，并设置消除神经网络中每层节点的概率，每一层中的每一个节点都以相应的概率消除，然后删除被消除节点的进出连线，得到一个节点数被精简的网络，并且对每一个剩余节点的参数做相应改变。

- 实现：对第 l 层，生成一个随机矩阵：dl=np.random.rand(al.shape[0],al.shape[1])<keep-prob， 然后用激活值矩阵乘以dl：al=np.multiply(al,dl)，接着al=al/keep-prob。

  > keep-prob (保存某个节点的概率)

- 为什么al要除以keep-prob？

  在使用dropout过程中，保留了与keep-prob对应的节点数（比如keep-prob=0.8，则保留了80%的节点数），所以al的期望值肯定变小了，为了保证al的期望值不变，就需要除以keep-prob。

  dropout只是在训练模型时使用，在进行测试时并不使用，因为如果测试时也是用，预测结果就是随机的了，那么训练的参数就没有意义了。

- 实施dropout在计算机视觉领域有很多成功的第一次，计算机视觉中的输入量非常大，输入了太多像素，以至于没有足够的数据，所以dropout在计算机视觉中应用得比较频繁，有些计算机视觉研究人员非常喜欢用它，几乎成了默认的选择。

  但是要牢记一点，dropout是一种正则化方法，它有助于预防过拟合，因此，除非算法过拟合，不然一般是不会使用dropout的。所以它在其它领域应用得比较少，主要应用在计算机视觉中，因为通常没有足够的数据，所以一直存在过拟合，这就是有些计算机视觉研究人员如此钟情dropout函数的原因。

- dropout一大缺点是代价函数J 不再被明确定义，每次迭代都会随机移除一些节点，如果再检查梯度下降的性能，实际上是很难进行复查的。定义明确的代价函数J每次迭代后都会下降。因为我们所优化的代价函数J实际上并没有明确定义，或者在某种程度上很难计算，我们失去了调试工具来绘制损失函数图。

## 1.8 其他正则化方法

1. 数据扩增

   图像的翻转，裁剪；

   数字图片的扭曲和旋转变形。

2.  Early stopping 及时结束迭代

   当开发集误差达到最低点（有回升的迹象时）停止；但是这时的代价函数值可能还不够低，也就意味着可能导致模型高偏差。

![image-20210915185116603](https://gitee.com/pinboy/typora-image/raw/master/img/202109151851665.png)

​	

总结： 以上都是聚焦于如何减少方差（用扩大数据量和及时停止训练 来减少过拟合， 正则化。） 

## 1.9正则化输入

处理：

① 零均值化：

μ = 1 / m * ∑x(i)

x = x - μ

![image-20210915193318935](https://gitee.com/pinboy/typora-image/raw/master/img/202109151933977.png)

② 方差归一化：

σ² = 1 / m * ∑(x(i))²

x = x / σ²

![image-20210915193326948](https://gitee.com/pinboy/typora-image/raw/master/img/202109151933976.png)



直观理解，正则化前后数据分布特点

![image-20210915190629032](https://gitee.com/pinboy/typora-image/raw/master/img/202109151906081.png)

正则化梯度下降的区别![image-20210915190645826](https://gitee.com/pinboy/typora-image/raw/master/img/202109151906879.png)

如果输入特征值之间的范围大不相同，就会导致各个参数的范围也大不相同，从而导致成本函数关于参数的函数图像在空间上表现为非常狭长的几何体，这样一来，如果你从距离中心非常远的点开始走，要走很久才能走到中心，如果这个几何体是一个截面为圆形的抛物面，无论你从哪里走，都能花相对少的时间走到中心，这也就加速了模型的训练速度。



## 1.10 梯度爆炸和梯度消失

假如有如下图示的深度神经网络：

![image-20210915192001852](https://gitee.com/pinboy/typora-image/raw/master/img/202109151920891.png)
为了直观理解梯度消失和梯度爆炸，我们假设所有激活函数为线性激活函数，即g(z)=zg(z)。并假设前L−1个权重矩阵都相等,即为$W_{linear}$，所以可以得到 $y_{hat}=W_{linear}^{L-1}W_{L}X$

假设$W_{linear}$都等于$y_{hat}=1.5^{L-1}W_LX$，很显然当L很大时则会出现梯度爆炸。

![image-20210915193409695](https://gitee.com/pinboy/typora-image/raw/master/img/202109151934740.png)



解决方案：权重初始化

由 z = w~1~x~1~ + w~2~x~2~ + ... + w~n~x~n~

随着 n 的增大，期望的 w^[l]^ 越小，由此设置 Var(w^[l]^) = 1/n 或者 2/n（效果更好），即：

>  w[l] = np.random.randn(shape) * np.sqrt(2/n[l-1])

详细推导见下：

https://www.cnblogs.com/marsggbo/p/7462682.html



## 1.12 梯度的数值逼近

梯度其实就是微分求导，但是由于求导结果可能比较复杂，所以使用微积分中的求导的定义来计算梯度。

$\frac{\partial J(θ)}{\partial θ}= \frac{J(θ+ε)-J(θ+ε)}{2ε}$

## 1.13 梯度检验

通过梯度检验，可以检验模型中是否有bug，如果梯度算出来误差较大，则需要考虑代码中是否有bug了。

通过下面图示的方法计算出$dθ_{approx}$，与公式计算的$dθ$较。

![image-20210915195519014](https://gitee.com/pinboy/typora-image/raw/master/img/202109151955099.png)

然后计算二者之间的距离（欧式距离->归一化）



## 1.14 梯度检验实际建议

- 1.上面提到的梯度检测只是用来让你判断反向传播算法是否正常，所以并不能用在训练模型的算法中，因为它运行效率真的很慢
- 2.如果梯度检测发现有问题，那么需要定位到误差较大的那一层进行debug(虽然并不太好定位)
- 3.如果损失函数使用了正则化，那么在计算梯度逼近值的时候也请记得加上正则项
- 4.不要和Dropout一起使用。因为Dropout会随机删除节点，所以根本就不容易计算梯度，也就是说梯度是会随机变化的。所以如果你的代码中已经使用了Dropout，那么当你运行梯度检测的时候，记得将Dropout的keepProb参数设为 1  ,这样就不会删除节点了。





参考：

https://mooc.study.163.com/learn/2001281003?tid=2403042000#/learn/content?type=detail&id=2403380407

https://www.cnblogs.com/orangecyh/p/11810840.html

https://blog.csdn.net/qq_xuanshuang/article/details/111202404

https://zhuanlan.zhihu.com/p/73513425