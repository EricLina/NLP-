https://blog.csdn.net/weixin_41043240/article/details/79289306

https://www.bbsmax.com/A/QW5YNaMedm/

## 3.1  调试处理

### 超参数有哪些呢？

深度神经网络的超参有学习速率α、层数、隐藏层单元数、mini-batch大小、学习速率衰减、 β（优化算法）等。
按重要性分类的话：

1. 最重要的参数就是学习速率α
2. 隐藏层单元数、mini-batch大小、 β（优化算法）
3.   层数、学习速率衰减


### 如何选择参数组合呢？

- ​	超参数较少的时候，一般是直接在参数组成的网格中 规则 采样，选出表现最好的一组。（遍历）（但是这个在参数较多的时候就不适用了）。

  在深度学习中，一般不用“规则”采样，而是随机采样。因为规则采样意味着超参数组合是固定的，能够检验的组合是固定的，有很多其他超参数组合无法通过规则采样遍历到。而用随机采样后，可以有机会检验每一个组合（虽然说检验总次数不变，但检验范围扩大了）

- 另一种策略就是逐步锁定区域，即先随机粗糙地选点，然后锁定几个表现优良的区域，在这几个区域中再来详细地选择。
- 所以可取的办法是，在一定的尺度范围内随机取值，先寻找一个较好的参数，再在该参数所在的区域更精细的寻找最优参数。



超参数的调试随机取值可以提升你的搜索效率，但随机取值并不一定是在有效值范围内的随机均匀取值，而是要选择合适的标尺。

### 如何选取合适的超参数范围？

- 有些超参数可以随机均匀取值

  假设 n[l] 可选取值 50~100：在整个范围内随机均匀取值。

  选取神经网络层数 Layers，L的可选取值为 2~4：在整个范围内随机均匀取值

- 还有一些超参数不能随机均匀取值，要选择合适的标尺

  

  学习速率 α 的可选取值 0.0001~1：但是如果不加限制随机取值，会导致0.1 ~ 1 的取值概率是90%,而0.0001 ~ 0.1 的取值概率是10%，这不太合理。所以解决办法是用一个标尺法将范围分为四部分，在每一个部分上随机均匀取值。

  ![img](https://gitee.com/pinboy/typora-image/raw/master/img/202109161506620.png)

  同理，β 的可选取值 0.9~0.999：分段，在 1-β 的对数轴上随机均匀取值

  ![image-20210916152949615](https://gitee.com/pinboy/typora-image/raw/master/img/202109161529646.png)

  > 为什么不直接取随机数呢？
  >
  > 因为直接取随机数的前提是：取0.00001到1之间的任何数，对于结果影响的灵敏度都是一样的。
  >
  > 但是其实当α越接近1的时候，所得结果的灵敏度就会越大。所以在靠近1的部分，我们要分配更多的检测几率。
  >
  > （比如1/(1-β)在 β接近1的时候，对结果的影响灵敏度就会很大了。）

## 3.3  Pandas vs Caviar 超参数重新评估方式

Pandas： 一组超参数，每天对超参数进行手工修改。（计算能力不足）

Caviar： 自动训练，画出学习曲线，然后换一组超参数。（有算力）

关于搜索超参数问题，大概分为两种思想流派。一种是熊猫方式（熊猫一次只能生一个仔），这种方法在计算资源不足时使用，即一次只调试一个模型，然后根据训练结果不断修改超参数。另外种是鱼子酱方式（鱼一次可以生很多仔），即一次调试很多个模型，不同模型超参数不同，最终选择表现最好的模型。。

这两种方式的选择，取决于所拥有的的计算资源及应用。如果拥有足够强大的计算资源，那么鱼子酱模式是首选，但是如果计算资源不足够，或一些数据量非常巨大的应用，如在线广告设置及计算机视觉领域，熊猫方式通常更合适。 在熊猫方式中如果一段时间后发现也许重新建立一个模型更好，也可以重新建立一个模型。![image-20210916212651261](https://gitee.com/pinboy/typora-image/raw/master/img/202109162126357.png)

## 3.2 batch归一化

Batch Normalization，它会使得超参数的搜索问题变得简单，使得神经网络对超参数的选择更加稳定，使得更加容易训练更加深层的网络。我们在这门课第一周第1讲学习过如何对logistic的输入数据进行归一化，那么Batch Normalization就是把这种归一化运用到深层神经网络、运用到神经网络每一个隐藏层。

![image-20210918092545091](https://gitee.com/pinboy/typora-image/raw/master/img/202109180925155.png)

其中 γ 和 β 为学习参数，这样就可以随意设置 Z~(i)~ 的平均值了。

batch norm 的作用是使隐层单元(对m个样本)的均值和方差标准化，使![Z^{[l]}](https://gitee.com/pinboy/typora-image/raw/master/img/202109180932289.gif)的每一行有固定的均值和方差，具体的均值和方差取值由![\beta^{[l]},\gamma^{[l]}](https://private.codecogs.com/gif.latex?\beta^{[l]}%2C\gamma^{[l]})决定，学习算法可以把![\beta^{[l]},\gamma^{[l]}](https://private.codecogs.com/gif.latex?%5Cbeta%5E%7B%5Bl%5D%7D%2C%5Cgamma%5E%7B%5Bl%5D%7D)更新为任意值。 每一层的β γ 都不一样，以此让每一层的分布不同(同属于正态分布，但是均值和方差不同)

这样有什么用呢？举个例子，若想要充分利用sigmoid 函数，那么我们输入的分布离0越远越好，所以方差就需要大一点比较好，这时候就可以通过改变β γ 来实现方差的缩放。



![image-20210919203149405](https://gitee.com/pinboy/typora-image/raw/master/img/202109192031453.png)



### 3.3 batch 拟合进神经网络

传播过程

![image-20210919210032380](https://gitee.com/pinboy/typora-image/raw/master/img/202109192100439.png)

当然也可以使用高级优化算法，如Adam，Momentum，RMSprop



## 3.4 为什么BatchNorm奏效

1. 之前我们曾将输入特征进行归一化（每个特征的取值范围差距很大），使每个特征的取值范围相近，起到了加速神经网络训练的效果。Batch Norm也采用了类似的原理，对每个隐层也做了类似的归一化，同理也能起到加速的效果。

对于[i+1]层来说，[i]层[提供的参数就是a[i] _[n]（n个参数）  ，而a[i] _[n] 又会随着[i-1]层变化，BatchNorm的作用就是保证a[i] _[n] 的变化始终保持在一个合理的范围内（比如均值始终是0，方差不大），这样就a[i] _[n] 的变化就不会对下一层造成太大的影响。

2. 降低了前面层参数的作用与后面层参数的作用之间的联系，使网络每一层都可以自己学习，稍稍独立于其他层，有助于加速整个网络的学习。对于后层来说，前层的输出(后层的输入)不会左右移动很多，因为它会被相同的均值和方差限制，使后层的学习变得更容易些。

3. BatchNorm相当于在每一层加入了一点噪声，使得每一层不会依赖于某一个特定的神经元

注意，BatchNorm不是一种正则化方法，他只是有一点正则化作用，也就是说不能用BatchNorm解决过拟合问题。



## 3.5 测试时 使用BatchNorm

由于测试时每次可能只处理一个数据，所以对于一个数据而言是没有均值和方差之说的，那么这时候在进行归一化时用到的均值和方差怎么获取呢？

在训练模型的时候，每一个minibatch对神经网络的第 L 层都会依次产生一个均值和方差，那么对所有minibatch在第L层得到的均值和方差求**指数平均**，这个指数平均就作为第 L 层的均值和方差，作为测试时使用。而测试时需用到的γ和β，就像W一样，是训练后的最优值。

## 3.6 多值分类器-softMax Regression

之前我们学习的分类都是2分类，softmax回归可以实现多分类

以四分类为例子：

![image-20210919213150527](https://gitee.com/pinboy/typora-image/raw/master/img/202109192131629.png)



> **softmax直白来说就是将原来输出是3,1,-3通过softmax函数一作用，就映射成为(0,1)的值，而这些值的累和为1（满足概率的性质），那么我们就可以将它理解成概率，在最后选取输出结点的时候，我们就可以选取概率最大（也就是值对应最大的）结点，作为我们的预测目标！**

##### softMax 求梯度

一般用交叉熵作为损失 函数

https://blog.csdn.net/bitcarmanlee/article/details/82320853

https://segmentfault.com/a/1190000017320763

##### softmax VS k个二元分类器

​	如果类别之间是互斥的，更适于选择softmax回归分类器（因为分类结果是一个特定类别）

​	如果类别之间不是互斥的，更适合使用 多个Logistics回归分类器（因为每一个类别，都会返回一个判定结果）



## 3.7 TensorFlow使用举例：

##### 大致结构：

[1] 用tf.Variable定义参数（tensorflow参数使用运算符号就和常量使用运算符号一样方便）

[2] 定义代价函数

[3] 指定代价函数的优化目标，如tf.train.GradientDescentOptimizer

[4] session.run(init) 初始化全局变量

[5] session.run(train) 使用优化算法进行参数的更新

##### 例子：

最小化 J = (w - 5)² = w² - 10w + 25:

```
import tensorflow as tf
import numpy as np

coefficients = np.array([[1], [-10], [25]])  # 相当于训练集

w = tf.Variable([0], dtype=tf.float32)  # 初始化模型参数
x = tf.placeholder(tf.float32, [3,1]) # placeholder创建占位符 ，实际计算时在传入数据
cost = x[0][0] * w ** 2 + x[1][0] * w + x[2][0]  # 定义代价函数
train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)  # 选择一个优化算法最小化代价函数 定义了一次迭代更新参数的过程

init = tf.global_variables_initializer()
session = tf.Session()
session.run(init)
print(session.run(w))
for i in range(1000):
    session.run(train)
print(session.run(w))


```





