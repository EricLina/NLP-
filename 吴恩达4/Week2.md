## 2.1 Min-batch梯度下降

### 背景：

 数据量比较大，传统梯度下降导致模型训练的时间过大。所以按照分类小批次处理的思想，我们对数据作划分。x^{1}^={x^(1)^,x^(2)^……x^(1000)^}， 其中x^(t)^ 表示划分后的mini-batch![image-20210916103237761](https://gitee.com/pinboy/typora-image/raw/master/img/202109161032841.png)

注意区分该系列教学视频的符号标记：

- **小括号()** 表示具体的某一个元素，指一个具体的值，例如x^(i)^x^(i)^
- **中括号[]** 表示神经网络中的某一层,例如Z^[l]^Z^[l]^
- **大括号{}** 表示将数据细分后的一个集合,例如x^{1}^={x^(1)^,x^(2)^……x^(1000)^}



### 算法流程：

假设我们有5,000,000个数据，每1000作为一个集合，计入上面所提到的x^{1}^={x^(1)^,x^(2)^……x^(1000)^}, 

需要迭代运行5000次神经网络运算。每一次迭代的过程如下

1. 前向传播（每次计算的数量是1000）　

   ​	　Z[1] = W[1]X{t} + b[1]

   　　A[1] =g[1] (Z[1])

   　　...

   　　A[L] = g[L](Z[L])

2. 计算损失函数，有正则化要加上正则项![image-20210916104507367](https://gitee.com/pinboy/typora-image/raw/master/img/202109161045403.png)

3. 反向传播   W[l] = W[l] - α * dW[l]

   ​					b[l] = b[l] - α * db[l]

mini-batch相比于之前一次性计算所有数据不仅速度快，而且反向传播需要计算5000次，所以效果也更好。

![image-20210916104004971](https://gitee.com/pinboy/typora-image/raw/master/img/202109161040053.png)

## 2.2 理解mini-batch

如上面所提到的，我们以1000位单位对数据进行划分，但是这只是为了更方便说明问题才这样划分的，那么我们在实际操作中应该如何划分呢？

考虑两个极端情况：

- **mini-batch size = m **  （蓝色线条）
  此时即为**Batch gradient descent**(批量梯度下降)，(x^{t}^,y^{t}^)=(X,Y)
- **mini-batch size = 1  ** （紫色）
  此时即为**Stochastic gradient descent**（随机梯度下降）, (x^{t}^,y^{t}^)=(x^(i)^,y^(i)^)

![image-20210916104627020](https://gitee.com/pinboy/typora-image/raw/master/img/202109161046066.png)

蓝色收敛曲线：比较耗时，但是最后能够收敛到最小值；

紫色收敛曲线：速度比较快，但是收敛曲线比较曲折，最终不一定能够收敛到最小点，而是在其附件波动。

### mini-batch选取的原则：

- 如果数据量比较小（m<2000），可以使用batch gradient descent。一般来说**mini-batch size取2的次方比较好**，例如64,128,256,512等，因为这样与计算机内存设置相似，运算起来会更快一些。

## 2.3  指数的加权平均(为后面的算法做准备)

指数加权平均也叫作指数加权移动平均。

### 举个栗子：

下图是一年的温度数据,要进行数据的拟合。![image-20210916104955652](https://gitee.com/pinboy/typora-image/raw/master/img/202109161049699.png)

现在需要计算出一个温度趋势曲线，计算方法如下：

-  V~t~ = βV~t-1~ + (1-β)θ~t~
- 上面的θ~t~表示第t天的温度，β是可调节的参数，
- 注意！V~t~可以视为 是最近  $1/(1−β)$天的每日温度。

比如：![image-20210916105204015](https://gitee.com/pinboy/typora-image/raw/master/img/202109161052049.png)



当β很大时，相当于对θ~t~ 赋权很小，也就是说当日的影响对V~t~ 的影响度不大，平均值的改变速度也会随之减慢。

而且当β接近1时，过去数据的权值会收敛，β = 0.98时，0.985^50^≈ 1/e,所以过去时刻数据的权值是会收敛到1/e 的。

下面是β取不同值时候的效果：

![image-20210916110901234](https://gitee.com/pinboy/typora-image/raw/master/img/202109161109292.png)

![image-20210916110851784](https://gitee.com/pinboy/typora-image/raw/master/img/202109161108843.png)

![image-20210916110842380](https://gitee.com/pinboy/typora-image/raw/master/img/202109161108435.png)

## 2.4 如何理解指数加权平均

![image-20210916115812514](https://gitee.com/pinboy/typora-image/raw/master/img/202109161158578.png)

可以看到在计算第t天的加权温度时，也将之前的温度考虑进来，但是都有一个衰减因子β，并且随着天数的增加，衰减幅度也不断增加。（https://blog.csdn.net/weixin_36811328/article/details/83451096） 

为什么不直接使用前若干项的值求平均值呢？因为存在大数据量的情况，无法一次性读取大量数据，指数加权平均可以仅从1项开始计算.

### 偏差修正

当β取0.98时，是紫色曲线，但是有一个问题，紫色曲线的起点比较低，如何解决呢？用偏差修正。

![image-20210916112352732](https://gitee.com/pinboy/typora-image/raw/master/img/202109161123852.png)



> 为什么β=0.98时起点比较低？
>
> 当计算移动平均数时，初始化 V0 = 0，V1 = 0.98 * V0 + 0.02 θ1 = 0.02 θ1.因此数据的前几项会比较小，不符合预测结果.
>
> ![image-20210916112537771](https://gitee.com/pinboy/typora-image/raw/master/img/202109161125838.png)

如何偏差修正？

- 使用公式$V_t=\frac{βV_{t-1}+(1-β)θ_t}{1-β^t}=\frac{V_{t}}{1-β^t}$来修正误差，

当t比较小的时候，前几项其实是偏小的，需要放大。比如t=2，$\frac{V_{t}}{1-β^t}=\frac{V_{t}}{1-0.98^2}=\frac{V_{t}}{0.0396}$修正了误差。

当t比较大的时候，其实不需要修正误差，分母取接近1就好，公式也满足条件$\frac{V_{t}}{1-β^t}≈\frac{V_{t}}{1}$

## 2.6  动量梯度下降法

https://blog.csdn.net/weixin_36811328/article/details/83451096

一般的梯度下降算法收敛情况：![image-20210916113326190](https://gitee.com/pinboy/typora-image/raw/master/img/202109161133238.png)

其实是走了不少弯路的，如果希望纵向不要跳的太多，横向走得快一点，我们可以使用动量梯度下降法。先看一下效果![image-20210916113700322](https://gitee.com/pinboy/typora-image/raw/master/img/202109161137367.png)

使用指数加权平均之后梯度代替原梯度进行参数更新。因为每个指数加权平均后的梯度含有之前梯度的信息，动量梯度下降法因此得名。

### 动量梯度下降法过程

算法过程中，涉及到两个超参数 α，β，其中β一般取0.9

1. 计算 $dw 和 db $(用minibatch)    

   > 梯度下降法的预设函数是$z=wx+b$ ,损失函数表示为$J(w,b)$ ,反向传播也需要计算dw和db

2. $V_{dw}=βV_{dw}+(1-β)dw$     (dw 的移动平均数）
   $V_{db}=βV_{db}+(1-β)db$      （db的移动平均数）

   > 如果不用动量梯度下降法，那么V~dw~就是一个普通的求导dw； V~db~就是一个普通的求导db；每次更新仅与当前梯度值相关，并不涉及之前的梯度。

3. $W=W-αV_{dw},b=b-αV_{db}$

   > 这就是梯度下降法正常的更新方式



所以为什么动量梯度下降法能够减少纵向波动，加快横向速度呢？

​	因为动量梯度下降法每一次反向传播，参考的不是本次的梯度，而是参考了前面1/（1-β）次的平均梯度。我们可以看到由于前面1/(1-β)次的梯度总是波折的，这就意味着梯度随时间变化是正，负，正，负，正，负，这样的分布，最后加起来平均就是一个趋于0的数。我们使用这样一个纵向梯度趋于0的数进行反向传播，这样就减少了纵向的波动。



## RMSprop算法

RMSProp算法的全称叫 Root Mean Square Prop，是Geoffrey E. Hinton在Coursera课程中提出的一种优化算法，在上面的Momentum优化算法中，虽然初步解决了优化中摆动幅度大的问题。所谓的摆动幅度就是在优化中经过更新之后参数的变化范围，如下图所示，蓝色的为Momentum优化算法所走的路线，绿色的为RMSProp优化算法所走的路线。

![image-20210916140114676](https://gitee.com/pinboy/typora-image/raw/master/img/202109161401723.png)

为了进一步优化损失函数在更新中存在摆动幅度过大的问题，并且进一步加快函数的收敛速度，RMSProp算法对权重 W 和偏置 b 的梯度使用了微分平方加权平均数。

算法流程如下：

1. 计算该次mini-batch的dw,db

2. $S_{dw}=β_2S_{dw}+(1-β_2)dw^2$
   $S_{db}=β_2S_{db}+(1-β_2)db^2$

3. $w:=w-α\frac{dw}{\sqrt{S_{dw}}+ε}$

   $b:=b-α\frac{db}{\sqrt{S_{db}}+ε}$

**RMSProp算法不是像AdaGrad算法那样暴力直接的累加平方梯度，而是加了一个衰减系数来控制历史信息的获取多少\***

RMSProp算法对梯度计算了 微分平方加权平均数。这种做法有利于消除了摆动幅度大的方向，用来修正摆动幅度，使得各个维度的摆动幅度都较小。另一方面也使得网络函数收敛更快。

（比如当 dW 或者 db 中有一个值比较大的时候，那么我们在更新权重或者偏置的时候除以它之前累积的梯度的平方根，这样就可以使得更新幅度变小）。为了防止分母为零，使用了一个很小的数值 $ϵ$ 来进行平滑，一般取值为 10^−8^。


## Adam算法

Adam算法结合了动量梯度下降法和RMSprop

有了上面两种优化算法，一种可以使用类似于物理中的动量来累积梯度，另一种可以使得收敛速度更快同时使得波动的幅度更小。

算法流程如下：

1. 计算dw，db

2. $V_{dw}=β_1V_{dw}+(1-β_1)dw$

   $V_{db}=β_1V_{db}+(1-β_1)db$

   $S_{dw}=β_2S_{dw}+(1-β_2)dw^2$

   $S_{db}=β_2S_{db}+(1-β_2)db^2$
   

3. $V_{dw}^{corrected}=\frac{V_{dw}}{1-β_1^t}$

   $S_{dw}^{corrected}=\frac{S_{dw}}{1-β_2^t}$

   > 这里就用到了指数平均法的偏差修正，主要解决的是迭代初期的值偏差大的问题。

   $W=W-α\frac{V_{dw}^{corrected}}{\sqrt{S_{dw}^{corrected}}+ε}$
   $b=b-α\frac{V_{db}^{corrected}}{\sqrt{S_{db}^{corrected}}+ε}$

算法流程汇总出现的超参数有 $α,β_1,β_2,ε$，一般$β_1=0.9,β2=0.999，ε=10^-8$

在训练开始时，初始化梯度累积量$V_{dw},V_{db},S_{dw},S_{db}=0$ 



## 2.9学习率衰减

如何设置学习率α，如果α设置为常数，则会出现这样的问题：对于刚开始收敛速度刚好，但是到后面α又显得太大，不能够完全收敛，而是在最低点来回波动。

为了解决这个问题，我们需要把学习率随着迭代次数逐渐衰减。（就像往水杯里倒水，最后要满了的时候，一般都会减慢速度小心地倒）

常见的衰减计算公式：

- $α=\frac{1}{1+DecayRate*epoch_{num}}α_0$

  **DecayRate**:衰减率
  **epoch_num**: 迭代次数

- ![image-20210916143532562](https://gitee.com/pinboy/typora-image/raw/master/img/202109161435606.png)



## 2.10 局部最优解的问题

当我们计算dw，db得到dw，db=0的时候，会是什么情况呢？一种直观上的想法，可能我们已经达到了局部最优解。

但是事实上，在很多参数条件下的高维空间，局部最优解出现的概率很小很小，相反，我们遇到鞍点的概率会比较大。

鞍点相比于局部最优点要更加棘手，因为从横向上看似乎是最低点，但是纵向上看却不是最低点。横向收敛只能沿着红色线的方向收敛直到鞍点，但是在沿着朝着鞍点横向下降的过程中，梯度又比较小（平稳段），会使学习很慢。 

![image-20210916143651690](https://gitee.com/pinboy/typora-image/raw/master/img/202109161436749.png)

**Adam,momentum,RMSprop可以加速通过平稳点的进程。**



